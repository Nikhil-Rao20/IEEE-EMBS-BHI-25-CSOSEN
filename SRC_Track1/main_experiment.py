"""
üöÄ Main Execution Script - Complete Model Experimentation Pipeline
================================================================

This script orchestrates the complete experimental pipeline from data loading
to conference-ready results compilation.

Execution Phases:
1. Data Loading & Preparation
2. Phase 1: Baseline Models
3. Phase 2: Classical ML Models  
4. Phase 3: Advanced Ensembles
5. Phase 4: Deep Learning Models
6. Phase 5: Time-Series Models
7. Statistical Analysis
8. Results Compilation

Author: Research Team
Date: September 2025
Purpose: IEEE EMBS BHI 2025 Conference Submission
"""

import pandas as pd
import numpy as np
import sys
import os
from pathlib import Path
import time
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# Add current directory to path to import local modules
sys.path.append(str(Path(__file__).parent))

# Import all phase modules
from experiment_framework import ExperimentFramework
from phase1_baseline_models import Phase1BaselineModels
from phase2_classical_ml import Phase2ClassicalML
from phase3_advanced_ensembles import Phase3AdvancedEnsembles
from phase4_deep_learning import Phase4DeepLearning
from phase5_timeseries import Phase5TimeSeriesModels
from statistical_analysis import StatisticalAnalysis
from results_compilation import ResultsCompilation

def main():
    """Main execution function."""
    
    print("üöÄ STARTING COMPLETE MODEL EXPERIMENTATION PIPELINE")
    print("=" * 65)
    print(f"üìÖ Start Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    
    # ============================================================================
    # SETUP AND DATA LOADING
    # ============================================================================
    
    print("üìã PHASE 0: SETUP AND DATA LOADING")
    print("-" * 40)
    
    # Initialize experiment framework
    output_folder_name = "Results_24W"  # ‚úÖ CHANGED 
    framework = ExperimentFramework(random_seed=42, output_folder_name=output_folder_name)
    
    # Data paths
    train_data_path = "../Track1_Data/processed/train_corrected_features.xlsx"
    
    # Check if processed data exists
    if not Path(train_data_path).exists():
        print("‚ùå Processed training data not found!")
        print("üí° Please run the feature engineering pipeline first.")
        print(f"   Expected path: {train_data_path}")
        return
    
    # Load data
    print("üìä Loading engineered features...")
    train_df, _ = framework.load_data(train_data_path)
    
    # Define target columns
    target_columns = ['bdi_ii_after_intervention_12w', 'bdi_ii_follow_up_24w']
    
    # Prepare data for experiments
    X, y = framework.prepare_data(train_df, target_columns)
    
    # Use primary target (12 weeks) for model comparison
    y_primary = y['bdi_ii_follow_up_24w']
    
    print(f"‚úÖ Data loaded successfully!")
    print(f"   Features: {X.shape[1]}")
    print(f"   Samples: {X.shape[0]}")
    print(f"   Target: BDI-II at 12 weeks post-intervention")
    print()
    
    # ============================================================================
    # EXPERIMENTAL PHASES
    # ============================================================================
    
    all_phase_results = {}
    phase_times = {}
    
    # Phase 1: Baseline Models
    print("üìã PHASE 1: BASELINE MODELS")
    print("-" * 35)
    start_time = time.time()
    
    try:
        phase1 = Phase1BaselineModels(random_seed=42)
        phase1_results = phase1.evaluate_all_models(X, y_primary, framework)
        
        if phase1_results:
            all_phase_results['phase1'] = phase1_results
            framework.save_results('phase1', phase1_results)
            
            print("\n" + phase1.generate_phase_summary())
        else:
            print("‚ö†Ô∏è Phase 1 produced no results")
            
    except Exception as e:
        print(f"‚ùå Phase 1 failed: {e}")
        phase1_results = {}
    
    phase_times['phase1'] = time.time() - start_time
    print(f"\n‚è±Ô∏è Phase 1 completed in {phase_times['phase1']:.1f} seconds")
    print()
    
    # Phase 2: Classical ML Models
    print("üìã PHASE 2: CLASSICAL ML MODELS")
    print("-" * 37)
    start_time = time.time()
    
    try:
        phase2 = Phase2ClassicalML(random_seed=42)
        phase2_results = phase2.evaluate_all_models(X, y_primary, framework)
        
        if phase2_results:
            all_phase_results['phase2'] = phase2_results
            framework.save_results('phase2', phase2_results)
            
            print("\n" + phase2.generate_phase_summary())
        else:
            print("‚ö†Ô∏è Phase 2 produced no results")
            
    except Exception as e:
        print(f"‚ùå Phase 2 failed: {e}")
        phase2_results = {}
    
    phase_times['phase2'] = time.time() - start_time
    print(f"\n‚è±Ô∏è Phase 2 completed in {phase_times['phase2']:.1f} seconds")
    print()
    
    # Phase 3: Advanced Ensembles
    print("üìã PHASE 3: ADVANCED ENSEMBLE MODELS")
    print("-" * 42)
    start_time = time.time()
    
    try:
        phase3 = Phase3AdvancedEnsembles(random_seed=42)
        phase3_results = phase3.evaluate_all_models(X, y_primary, framework)
        
        if phase3_results:
            all_phase_results['phase3'] = phase3_results
            framework.save_results('phase3', phase3_results)
            
            print("\n" + phase3.generate_phase_summary())
        else:
            print("‚ö†Ô∏è Phase 3 produced no results")
            
    except Exception as e:
        print(f"‚ùå Phase 3 failed: {e}")
        phase3_results = {}
    
    phase_times['phase3'] = time.time() - start_time
    print(f"\n‚è±Ô∏è Phase 3 completed in {phase_times['phase3']:.1f} seconds")
    print()
    
    # Phase 4: Deep Learning Models
    print("üìã PHASE 4: DEEP LEARNING MODELS")
    print("-" * 37)
    start_time = time.time()
    
    try:
        phase4 = Phase4DeepLearning(random_seed=42)
        phase4_results = phase4.evaluate_all_models(X, y_primary, framework)
        
        if phase4_results:
            all_phase_results['phase4'] = phase4_results
            framework.save_results('phase4', phase4_results)
            
            print("\n" + phase4.generate_phase_summary())
        else:
            print("‚ö†Ô∏è Phase 4 produced no results")
            
    except Exception as e:
        print(f"‚ùå Phase 4 failed: {e}")
        phase4_results = {}
    
    phase_times['phase4'] = time.time() - start_time
    print(f"\n‚è±Ô∏è Phase 4 completed in {phase_times['phase4']:.1f} seconds")
    print()
    
    # Phase 5: Time-Series Models
    print("üìã PHASE 5: TIME-SERIES & TRAJECTORY MODELS")
    print("-" * 48)
    start_time = time.time()
    
    try:
        phase5 = Phase5TimeSeriesModels(random_seed=42)
        phase5_results = phase5.evaluate_all_models(X, y_primary, framework)
        
        if phase5_results:
            all_phase_results['phase5'] = phase5_results
            framework.save_results('phase5', phase5_results)
            
            print("\n" + phase5.generate_phase_summary())
        else:
            print("‚ö†Ô∏è Phase 5 produced no results")
            
    except Exception as e:
        print(f"‚ùå Phase 5 failed: {e}")
        phase5_results = {}
    
    phase_times['phase5'] = time.time() - start_time
    print(f"\n‚è±Ô∏è Phase 5 completed in {phase_times['phase5']:.1f} seconds")
    print()
    
    # ============================================================================
    # STATISTICAL ANALYSIS
    # ============================================================================
    
    print("üìä STATISTICAL ANALYSIS")
    print("-" * 25)
    start_time = time.time()
    
    try:
        if all_phase_results:
            statistical_analyzer = StatisticalAnalysis(alpha=0.05)
            
            # Generate comprehensive statistical report
            statistical_report = statistical_analyzer.generate_statistical_report(all_phase_results)
            print("\n" + statistical_report)
            
            # Create statistical visualizations
            statistical_analyzer.create_statistical_visualizations(
                all_phase_results, 
                save_plots=True, 
                output_dir=f"../{output_folder_name}/Statistical_Analysis"
            )
            
        else:
            print("‚ö†Ô∏è No results available for statistical analysis")
            statistical_analyzer = None
            
    except Exception as e:
        print(f"‚ùå Statistical analysis failed: {e}")
        statistical_analyzer = None
    
    stat_time = time.time() - start_time
    print(f"\n‚è±Ô∏è Statistical analysis completed in {stat_time:.1f} seconds")
    print()
    
    # ============================================================================
    # RESULTS COMPILATION
    # ============================================================================
    
    print("üìã RESULTS COMPILATION")
    print("-" * 25)
    start_time = time.time()
    
    try:
        if all_phase_results:
            print("üîß Initializing ResultsCompilation...")
            results_compiler = ResultsCompilation(output_folder_name=f"../{output_folder_name}")
            
            # Compile all results
            print("üîÑ Compiling all phase results...")
            results_compiler.compile_all_results(all_phase_results, statistical_analyzer)
            
            # Create publication tables
            print("\n[TABLES] Creating Publication Tables...")
            try:
                perf_table = results_compiler.create_performance_summary_table()
                phase_table = results_compiler.create_phase_comparison_table()
                stat_table = results_compiler.create_statistical_significance_table()
                clinical_table = results_compiler.create_clinical_significance_table()
                print("[SUCCESS] All publication tables created!")
            except Exception as table_error:
                print(f"[ERROR] Error creating tables: {table_error}")
                import traceback
                traceback.print_exc()
            
            # Create publication figures
            print("\n[FIGURES] Creating Publication Figures...")
            try:
                results_compiler.create_publication_figures()
                print("[SUCCESS] All 14 publication figures created!")
            except Exception as fig_error:
                print(f"[ERROR] Error creating figures: {fig_error}")
                import traceback
                traceback.print_exc()
            
            # Generate conference summary
            print("\n[SUMMARY] Generating Conference Summary...")
            try:
                conference_summary = results_compiler.generate_conference_summary()
                print("[SUCCESS] Conference summary generated!")
                print("\n" + conference_summary)
            except Exception as summary_error:
                print(f"[ERROR] Error generating summary: {summary_error}")
                import traceback
                traceback.print_exc()
            
            # Save all results to Conference_Submission
            print("\n[SAVE] Saving all results to Conference_Submission folder...")
            try:
                results_compiler.save_all_results()
                print("[SUCCESS] All results saved to Conference_Submission folder!")
                
                # Verify Conference_Submission folder was created
                conf_dir = Path(f"../{output_folder_name}") / "Conference_Submission"
                if conf_dir.exists():
                    print(f"[VERIFIED] Conference_Submission folder verified: {conf_dir}")
                    contents = list(conf_dir.glob('*'))
                    print(f"   Contents: {[f.name for f in contents]}")
                else:
                    print(f"[ERROR] Conference_Submission folder not found at: {conf_dir}")
                    
            except Exception as save_error:
                print(f"[ERROR] Error saving results: {save_error}")
                import traceback
                traceback.print_exc()
            
        else:
            print("‚ö†Ô∏è No results available for compilation")
            
    except Exception as e:
        print(f"‚ùå Results compilation failed: {e}")
        import traceback
        traceback.print_exc()
    
    compilation_time = time.time() - start_time
    print(f"\n‚è±Ô∏è Results compilation completed in {compilation_time:.1f} seconds")
    print()
    
    # ============================================================================
    # SAVE ALL TRAINED MODELS (RANKED BY R¬≤ SCORE)
    # ============================================================================
    
    print("üíæ SAVING ALL TRAINED MODELS")
    print("-" * 35)
    start_time = time.time()
    
    try:
        if all_phase_results:
            import pickle
            import json
            
            # Create All_Trained_Models directory
            all_models_dir = Path("../All_Trained_Models")
            all_models_dir.mkdir(exist_ok=True)
            
            # Collect all models with their performance for comprehensive ranking
            all_model_info = []
            for phase_name, phase_results in all_phase_results.items():
                for model_name, model_results in phase_results.items():
                    if 'mean_scores' in model_results:
                        mae = model_results['mean_scores']['test_mae']
                        r2 = model_results['mean_scores']['test_r2']
                        rmse = model_results['mean_scores']['test_rmse']
                        
                        # Handle models that may not have been serialized properly
                        models_list = model_results.get('models', [])
                        if not models_list:
                            print(f"      ‚ö†Ô∏è {phase_name}_{model_name}: Performance calculated but model objects missing")
                            print(f"         R¬≤={r2:.4f}, MAE={mae:.3f} - Will create metadata-only entry")
                        
                        all_model_info.append({
                            'phase': phase_name,
                            'model_name': model_name,
                            'full_name': f"{phase_name}_{model_name}",
                            'mae': mae,
                            'r2': r2,
                            'rmse': rmse,
                            'models': models_list,
                            'results': model_results,
                            'has_model_objects': len(models_list) > 0
                        })
            
            # Sort by R¬≤ (higher is better), then by RMSE (lower is better) as tiebreaker
            all_model_info.sort(key=lambda x: (-x['r2'], x['rmse']))
            total_models = len(all_model_info)
            
            print(f"üèÜ Saving ALL {total_models} Models (ranked by R¬≤ score, RMSE tiebreaker):")
            print(f"   üìä Top 5 Models:")
            for i, model in enumerate(all_model_info[:5], 1):
                print(f"      Rank {i:2d}: {model['full_name']:<35} R¬≤={model['r2']:.4f}, MAE={model['mae']:.3f}, RMSE={model['rmse']:.3f}")
            if total_models > 5:
                print(f"   ... and {total_models-5} more models")
            
            # Save each model with proper ranking
            saved_with_models = 0
            metadata_only = 0
            
            for i, model_info in enumerate(all_model_info, 1):
                model_dir = all_models_dir / f"Rank_{i:02d}_{model_info['full_name']}"
                model_dir.mkdir(exist_ok=True)
                
                if model_info['has_model_objects']:
                    # Save trained models (cross-validation folds)
                    models_folder = model_dir / "trained_models"
                    models_folder.mkdir(exist_ok=True)
                    
                    for fold_idx, trained_model in enumerate(model_info['models']):
                        try:
                            if hasattr(trained_model, 'save'):  # Keras models
                                model_path = models_folder / f"fold_{fold_idx}.keras"
                                trained_model.save(model_path)
                            elif hasattr(trained_model, 'save_model'):  # XGBoost, etc.
                                model_path = models_folder / f"fold_{fold_idx}.model"
                                trained_model.save_model(model_path)
                            else:  # Sklearn models
                                model_path = models_folder / f"fold_{fold_idx}.pkl"
                                with open(model_path, 'wb') as f:
                                    pickle.dump(trained_model, f)
                        except Exception as e:
                            print(f"      ‚ö†Ô∏è Could not save fold {fold_idx} for {model_info['full_name']}: {e}")
                    
                    saved_with_models += 1
                else:
                    # Create metadata-only entry for models without objects (typically Phase 5)
                    models_folder = model_dir / "trained_models"
                    models_folder.mkdir(exist_ok=True)
                    
                    # Create README explaining missing models
                    readme_content = f"""# {model_info['model_name'].upper()} - PERFORMANCE CALCULATED

‚ö†Ô∏è MODEL TRAINING COMPLETED BUT MODEL OBJECTS NOT SAVED

## Performance (Rank #{i})
- R¬≤ Score: {model_info['r2']:.4f}
- MAE: {model_info['mae']:.3f}
- RMSE: {model_info['rmse']:.3f}

## Status
‚úÖ Cross-validation completed successfully
‚úÖ Performance metrics calculated
‚ùå Model objects not serialized (typically deep learning models)

## Recovery
The model can be retrained using the same hyperparameters and data splits.
"""
                    
                    readme_file = model_dir / "README_PERFORMANCE_ONLY.txt"
                    with open(readme_file, 'w', encoding='utf-8') as f:
                        f.write(readme_content)
                    
                    metadata_only += 1
                
                # Save model metadata and performance
                metadata = {
                    'rank': i,
                    'phase': model_info['phase'],
                    'model_name': model_info['model_name'],
                    'full_name': model_info['full_name'],
                    'performance': {
                        'r2': float(model_info['r2']),
                        'mae': float(model_info['mae']),
                        'rmse': float(model_info['rmse'])
                    },
                    'cross_validation_scores': {
                        k: v.tolist() if hasattr(v, 'tolist') else v
                        for k, v in model_info['results']['cv_scores'].items()
                    },
                    'mean_scores': {
                        k: float(v) for k, v in model_info['results']['mean_scores'].items()
                    },
                    'std_scores': {
                        k: float(v) for k, v in model_info['results']['std_scores'].items()
                    },
                    'training_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                    'experiment_id': framework.experiment_id,
                    'ranking_criteria': 'R¬≤ score (descending), RMSE (ascending) tiebreaker',
                    'has_model_objects': model_info['has_model_objects'],
                    'total_folds': len(model_info['models']) if model_info['has_model_objects'] else 0
                }
                
                # Save metadata
                with open(model_dir / "model_metadata.json", 'w') as f:
                    json.dump(metadata, f, indent=2)
                
                # Save detailed training history if available (for deep learning models)
                if model_info['has_model_objects'] and model_info['models'] and hasattr(model_info['models'][0], 'history'):
                    training_history = {}
                    for fold_idx, trained_model in enumerate(model_info['models']):
                        if hasattr(trained_model, 'history'):
                            history_dict = trained_model.history.history
                            # Convert numpy arrays to lists for JSON serialization
                            history_serializable = {
                                k: [float(val) for val in v] if isinstance(v, list) else float(v)
                                for k, v in history_dict.items()
                            }
                            training_history[f'fold_{fold_idx}'] = history_serializable
                    
                    if training_history:
                        with open(model_dir / "training_history.json", 'w') as f:
                            json.dump(training_history, f, indent=2)
                
                print(f"   {i:2d}. {model_info['full_name']:<35} | R¬≤: {model_info['r2']:.4f} | MAE: {model_info['mae']:.3f} | RMSE: {model_info['rmse']:.3f}")
            
            # Create a comprehensive summary file for easy loading
            all_models_summary = {
                'experiment_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'experiment_id': framework.experiment_id,
                'ranking_criteria': 'R¬≤ score (descending), RMSE (ascending) tiebreaker',
                'total_models_processed': total_models,
                'models_with_objects': saved_with_models,
                'metadata_only': metadata_only,
                'best_model': {
                    'name': all_model_info[0]['full_name'],
                    'path': f"Rank_01_{all_model_info[0]['full_name']}",
                    'performance': {
                        'r2': float(all_model_info[0]['r2']),
                        'mae': float(all_model_info[0]['mae']),
                        'rmse': float(all_model_info[0]['rmse'])
                    }
                },
                'top_10_models': [
                    {
                        'rank': i,
                        'name': model['full_name'],
                        'path': f"Rank_{i:02d}_{model['full_name']}",
                        'r2': float(model['r2']),
                        'mae': float(model['mae']),
                        'rmse': float(model['rmse'])
                    }
                    for i, model in enumerate(all_model_info[:10], 1)
                ],
                'all_models': [
                    {
                        'rank': i,
                        'name': model['full_name'],
                        'path': f"Rank_{i:02d}_{model['full_name']}",
                        'r2': float(model['r2']),
                        'mae': float(model['mae']),
                        'rmse': float(model['rmse'])
                    }
                    for i, model in enumerate(all_model_info, 1)
                ],
                'loading_instructions': {
                    'keras_models': "tf.keras.models.load_model('path/to/model.keras')",
                    'sklearn_models': "pickle.load(open('path/to/model.pkl', 'rb'))",
                    'metadata': "json.load(open('path/to/model_metadata.json', 'r'))"
                }
            }
            
            with open(all_models_dir / "all_models_summary.json", 'w') as f:
                json.dump(all_models_summary, f, indent=2)
            
            print(f"\n‚úÖ ALL {total_models} models processed and ranked!")
            print(f"   üíæ Models with saved objects: {saved_with_models}")
            print(f"   üìã Metadata-only entries: {metadata_only}")
            print(f"   üìÅ Location: {all_models_dir}")
            print(f"ü•á Best model: {all_model_info[0]['full_name']}")
            print(f"   üìä Performance: R¬≤={all_model_info[0]['r2']:.4f}, MAE={all_model_info[0]['mae']:.3f}, RMSE={all_model_info[0]['rmse']:.3f}")
            
        else:
            print("‚ö†Ô∏è No results available for model saving")
            
    except Exception as e:
        print(f"‚ùå Model saving failed: {e}")
        import traceback
        traceback.print_exc()
    
    model_save_time = time.time() - start_time
    print(f"\n‚è±Ô∏è All models saving completed in {model_save_time:.1f} seconds")
    print()
    
    # ============================================================================
    # FINAL SUMMARY
    # ============================================================================
    
    print("üéâ EXPERIMENT PIPELINE COMPLETED!")
    print("=" * 40)
    
    total_time = sum(phase_times.values()) + stat_time + compilation_time + model_save_time
    total_models = sum(len(phase_data) for phase_data in all_phase_results.values())
    
    print(f"üìä EXECUTION SUMMARY:")
    print(f"   ‚Ä¢ Total execution time: {total_time:.1f} seconds ({total_time/60:.1f} minutes)")
    print(f"   ‚Ä¢ Total models evaluated: {total_models}")
    print(f"   ‚Ä¢ Successful phases: {len(all_phase_results)}/5")
    print(f"   ‚Ä¢ ALL models saved with R¬≤ ranking: ‚úÖ")
    
    print(f"\n‚è±Ô∏è PHASE TIMING:")
    for phase, phase_time in phase_times.items():
        models_in_phase = len(all_phase_results.get(phase, {}))
        avg_time_per_model = phase_time / models_in_phase if models_in_phase > 0 else 0
        print(f"   ‚Ä¢ {phase.title()}: {phase_time:.1f}s ({models_in_phase} models, {avg_time_per_model:.1f}s/model)")
    
    print(f"   ‚Ä¢ Statistical Analysis: {stat_time:.1f}s")
    print(f"   ‚Ä¢ Results Compilation: {compilation_time:.1f}s")
    print(f"   ‚Ä¢ Model Saving: {model_save_time:.1f}s")
    
    if all_phase_results:
        # Find overall best model by R¬≤ score
        all_model_scores = []
        for phase_name, phase_results in all_phase_results.items():
            for model_name, model_results in phase_results.items():
                if 'mean_scores' in model_results:
                    mae = model_results['mean_scores']['test_mae']
                    r2 = model_results['mean_scores']['test_r2']
                    rmse = model_results['mean_scores']['test_rmse']
                    all_model_scores.append((f"{phase_name}_{model_name}", mae, r2, rmse))
        
        if all_model_scores:
            # Sort by R¬≤ (higher is better), then by RMSE (lower is better)
            best_model = max(all_model_scores, key=lambda x: (x[2], -x[3]))
            print(f"\nüèÜ BEST OVERALL MODEL (by R¬≤ score):")
            print(f"   ‚Ä¢ Model: {best_model[0].replace('_', ' ').title()}")
            print(f"   ‚Ä¢ R¬≤: {best_model[2]:.4f}")
            print(f"   ‚Ä¢ MAE: {best_model[1]:.3f}")
            print(f"   ‚Ä¢ RMSE: {best_model[3]:.3f}")
            print(f"   ‚Ä¢ Saved in: ../All_Trained_Models/Rank_01_{best_model[0]}/")
    
    print(f"\nüìÅ RESULTS LOCATION:")
    print(f"   ‚Ä¢ Main results: ../Results/")
    print(f"   ‚Ä¢ Conference submission: ../Results/Conference_Submission/")
    print(f"   ‚Ä¢ ALL trained models: ../All_Trained_Models/ (ranked by R¬≤)")
    print(f"   ‚Ä¢ Statistical analysis: ../Results/Statistical_Analysis/")
    print(f"   ‚Ä¢ Model experiments: ../Results/Model_Experiments/")
    print(f"   ‚Ä¢ üÜï Top 10 trained models: ../Top10_Trained_Models/")
    
    print(f"\nüìã DELIVERABLES READY:")
    print(f"   ‚úÖ Performance comparison tables")
    print(f"   ‚úÖ Statistical significance analysis")
    print(f"   ‚úÖ Clinical significance assessment")
    print(f"   ‚úÖ Publication-ready figures")
    print(f"   ‚úÖ Conference submission summary")
    print(f"   ‚úÖ Complete experimental results")
    print(f"   ‚úÖ üÜï ALL trained models saved with R¬≤ ranking")
    print(f"   ‚úÖ üÜï Training curves and validation scores")
    print(f"   ‚úÖ üÜï RMSE added as tiebreaker metric")
    
    print(f"\nüéØ NEXT STEPS:")
    print(f"   1. Review results in ../Results/Conference_Submission/")
    print(f"   2. Load best model from ../All_Trained_Models/Rank_01_*/ for analysis")
    print(f"   3. Use saved models for production deployment (all models available)")
    print(f"   4. Customize tables and figures for target conference")
    print(f"   5. Write manuscript using provided results")
    print(f"   6. Submit to IEEE EMBS BHI 2025!")
    
    print(f"\nüìÖ Completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("üöÄ EXPERIMENT PIPELINE SUCCESSFULLY COMPLETED! üéâ")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è Experiment interrupted by user")
        print("üîÑ Partial results may be available in ../Results/")
    except Exception as e:
        print(f"\n\n‚ùå Experiment failed with error: {e}")
        print("üîç Check the error details above and retry")
        import traceback
        print("\nüìã Full traceback:")
        traceback.print_exc()